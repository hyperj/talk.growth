# 《人工智能基础课 - 王天一》

## 06 数学基础 | 明日黄花迹难寻：形式逻辑

- 理想的人工智能应该具备抽象意义上的学习、推理与归纳能力

- 人工智能的早期研究者认为人类认知和思维的基本单元是符号，而认知过程就是对符号的逻辑运算，这样一来，人类抽象的逻辑思维就可以通过计算机中逻辑门的运算模拟，进而实现机械化的人类认知。

    反过来，形式逻辑也是智能行为的描述方式，任何能够将某些物理模式或符号转化成其他模式或符号的系统都有可能产生智能的行为，也就是人工智能。人工智能能够模拟智能行为的基础是具有知识，但知识本身也是抽象的概念，需要用计算机能够理解的方式表示出来。

- 在人工智能中，常用的知识表示方法包括数据结构和处理算法。数据结构用于静态存储待解决的问题、问题的中间解答、问题的最终解答以及解答中涉及的知识；处理算法则用于在已有问题和知识之间进行动态交互，两者共同构成完整的知识表示体系。

- 在人工智能的研究中，用形式逻辑实现知识表示是一种普遍的方法。形式逻辑可谓包罗万象，其最简单的实例就是由古希腊哲学家亚里士多德提出并流传至今的三段论，它由两个前提和一个结论构成：

- 在人工智能中应用的主要是一阶谓词逻辑。谓词逻辑是最基本的逻辑系统，也是形式逻辑的根本部分。谓词逻辑的一个特例是命题逻辑。在命题逻辑中，命题是逻辑处理的基本单位，只能对其真伪做出判断。


**总结**

- 如果将认知过程定义为对符号的逻辑运算，人工智能的基础就是形式逻辑；
- 谓词逻辑是知识表示的主要方法；
- 基于谓词逻辑系统可以实现具有自动推理能力的人工智能；
- 不完备性定理向“认知的本质是计算”这一人工智能的基本理念提出挑战。

**术语**

- 谓词逻辑将命题拆分为个体词、谓词和量词，三者的意义如下：
    
    个体词：是可以独立存在的具体或抽象的描述对象；
    
    谓词：用于描述个体词的属性与相互关系；
    
    量词：用于描述个体词的数量关系，包括全称量词 $\forall$ 和存在量词 $\exists$。

- 逻辑联结词：

    否定($\neg$)：复合命题 $\neg P$ 表示否定命题 $P$ 的真值的命题，即“非 $P$ ” 。
    
    合取($\wedge$)：复合命题 $P \wedge Q$ 表示命题 $P$ 和命题 $Q$ 的合取，即“ $P$ 且 $Q$ ”。
    
    析取($\vee$)：复合命题 $P \vee Q$ 表示命题 $P$ 或命题 $Q$ 的析取，即“ $P$ 或 $Q$ ”。
    
    蕴涵($\to$)：复合命题 表示命题 $P \to Q$ 是命题 $Q$ 的条件，即“如果 $P$，那么$Q$”。
    
    等价($\leftrightarrow$)：复合命题 表示命题 $P \leftrightarrow Q$ 和命题 $Q$ 相互蕴涵，即“如果 $P$，那么 $Q$ 且如果 $Q$ ，那么 $P$ ”。
    


**Reference**

## 05 数学基础 | 万物皆数，信息亦然：信息论

- 信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。

**总结**

- 信息论处理的是客观世界中的不确定性；
- 条件熵和信息增益是分类问题中的重要参数；
- KL 散度用于描述两个不同概率分布之间的差异；
- 最大熵原理是分类问题中的常用准则。

**术语**

- 熵：本质是描述一个系统内在的混乱程度。
  
    在信息论中，如果事件 $A$ 发生的概率为 $p(A)$ ，则这个事件的自信息量的定义为
  
    $$h(A) = - \log_2 p(A)$$
  
    根据单个事件的自信息量可以计算包含多个符号的信源的信息熵。
    
- 信源熵：信源的信息熵是信源可能发出的各个符号的自信息量在信源构成的概率空间上的统计均值。如果一个离散信源 $X$ 包含 $n$ 个符号，每个符号 $a_i$ 的取值为 $p(a_i)$，则 $X$ 的信源熵为
  
    $$H(X) = -\sum\limits_{i = 1}^n p(a_i) \log_2 p(a_i)$$
    
    信源熵描述了信源每发送一个符号所提供的平均信息量，是信源总体信息测度的均值。当信源中的每个符号的取值概率相等时，信源熵取到最大值 $\log _2 n$ ，意味着信源的随机程度最高。

- 条件熵：如果两个信源之间具有相关性，那么在已知其中一个信源 $X$ 的条件下，另一个信源 $Y$ 的信源熵就会减小。条件熵 $H(Y|X) 表示的是在已知随机变量 $X$ 的条件下另一个随机变量 $Y$ 的不确定性，也就是在给定 $X$ 时，根据 $Y$ 的条件概率计算出的熵再对 $X$ 求解数学期望：
  
    $$H(Y|X) = \sum_{i = 1}^ n p(x_i) H(Y|X = x_i)$$
    
    $$= -\sum_{i = 1}^ n p(x_i) \sum_{j = 1}^ m p(y_j|x_i) \log_2 p(y_j|x_i)$$
    
    $$= - \sum_{i = 1}^ n \sum_{j = 1}^ m p(x_i, y_j) \log_2 p(y_j|x_i)$$
    
    条件熵的意义在于先按照变量 $X$ 的取值对变量 $Y$ 进行了一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照 $X$ 的分布计算其数学期望。

- 互信息：等于 $Y$ 的信源熵减去已知 $X$ 时 $Y$ 的条件熵，即由 $X$ 提供的关于 $Y$ 的不确定性的消除，也可以看成是 $X$ 给 $Y$ 带来的信息增益。互信息这个名称在通信领域经常使用，信息增益则在机器学习领域中经常使用，两者的本质是一样的。
    
    在机器学习中，信息增益常常被用于分类特征的选择。对于给定的训练数据集 $Y$ ，$H(Y)$ 表示在未给定任何特征时，对训练集进行分类的不确定性； $H(Y|X)$ 则表示了使用特征 $X$ 对训练集 $Y$ 进行分类的不确定性。信息增益表示的就是特征 $X$ 带来的对训练集 $Y$ 分类不确定性的减少程度，也就是特征 $X$ 对训练集 $Y$ 的区分度。
    
- 信息增益比：信息增益更大的特征具有更强的分类能力。但信息增益的值很大程度上依赖于数据集的信息熵 $H(Y)$ ，因而并不具有绝对意义。为解决这一问题，研究者又提出了信息增益比的概念，并将其定义为 g(X, Y) = I(X; Y) / H(Y)。

- KL（Kullback-Leibler）散度：KL 散度是描述两个概率分布 $P$ 和 $Q$ 之间的差异的一种方法，其定义为：
    
    KL 散度是对额外信息量的衡量。给定一个信源，其符号的概率分布为 $P(X)$ ，就可以设计一种针对 $P(X)$ 的最优编码，使得表示该信源所需的平均比特数最少（等于该信源的信源熵）。
    
    可是当信源的符号集合不变，而符合的概率分布变为 $Q(X)$ 时，再用概率分布 $P(X)$ 的最优编码对符合分布 $Q(X)$ 的符号编码，此时编码结果的字符数就会比最优值多一些比特。
    
    KL 散度就是用来衡量这种情况下平均每个字符多用的比特数，也可以表示两个分布之间的距离。
    
    KL 散度的两个重要性质是非负性和非对称性。
    
    非负性是指 KL 散度是大于或等于 0 的，等号只在两个分布完全相同时取到。
    
    非对称性则是指 $D_{KL}(P||Q) \ne D_{KL}(Q||P)$ ，即用 $P(X)$ 去近似 $Q(X)$ 和用 $Q(X)$ 去近似 $P(X)$ 得到的偏差是不同的，因此 KL 散度并不满足数学意义上对距离的定义，这一点需要注意。
    
    事实上， $D_{KL}(P||Q)$ 和 $D_{KL}(Q||P)$ 代表了两种不同的近似方式。要让 $D_{KL}(P||Q)$ 最小，需要让 $Q(X)$ 在 $P(X)$ 不等于 0 的位置同样不等于 0；要让 $D_{KL}(Q||P)$ 最小，则需要让 $Q(X)$ 在 $P(X)$ 等于 0 的位置同样等于 0。

- 最大熵原理：是确定随机变量统计特性时力图最符合客观情况的一种准则。对于一个未知的概率分布，最坏的情况就是它以等可能性取到每个可能的取值。这个时候的概率分布最均匀，也就是随机变量的随机程度最高，对它进行预测也就最困难。
    
    从这个角度看，最大熵原理的本质在于在推断未知分布时不引入任何多余的约束和假设，因而可以得到最不确定的结果，预测的风险也就最小。
    
    将最大熵原理应用到分类问题上就可以得到**最大熵模型**。在分类问题中，首先要确定若干特征函数作为分类的依据。为了保证特征函数的有效性，其在模型真实分布 $P(X)$ 上的数学期望和在由训练数据集推导出的经验分布 $\tilde P(X)$ 上的数学期望应该相等，即对给定特征函数数学期望的估计应该是个无偏估计量。
    
    这样一来，每一个特征函数就对应了一个约束条件。分类的任务就是在这些约束条件下，确定一个最好的分类模型。由于除了这些约束条件之外，没有任何关于分类的先验知识，因而需要利用最大熵原理，求解出不确定性最大的条件分布，即让以下函数的取值最大化
    
    $$H(p) = -\sum\limits_{x, y} \tilde p(x) p(y|x) \log_2 p(y|x)$$
    
    式中的 $p(y|x)$ 就是分类问题要确定的目标条件分布。计算上式的最大值实质上就是一个约束优化问题，由特征函数确定的约束条件可以通过**拉格朗日乘子**的引入去除其影响，转化为无约束优化问题。从数学上可以证明，这个模型的解是存在且唯一的。

**Reference**


## 04 数学基础 | 不畏浮云遮望眼：最优化方法

- 人工智能的目标就是最优化：在复杂环境与多体交互中做出最优决策

**总结**

- 通常情况下，最优化问题是在无约束情况下求解给定目标函数的最小值；
- 在线性搜索中，确定寻找最小值时的搜索方向需要使用目标函数的一阶导数和二阶导数；
- 置信域算法的思想是先确定搜索步长，再确定搜索方向；
- 以人工神经网络为代表的启发式算法是另外一类重要的优化方法。

**术语**

- 最优化理论（optimization）研究的问题是判定给定目标函数的最大值（最小值）是否存在，并找到令目标函数取到最大值（最小值）的数值

- 目标函数（objective function）或评价函数：求解最小化或最大化的函数
    
    全局最小值（global minimum）比定义域内所有其他点的函数值都小；局部极小值（local minimum）只是比所有邻近点的函数值都小

- 根据约束条件的不同，最优化问题可以分为无约束优化（unconstrained optimization）和约束优化（constrained optimization）两类。

    无约束优化对自变量 $x$ 的取值没有限制，约束优化则把 $x$ 的取值限制在特定的集合内，也就是满足一定的约束条件。
  
    线性规划（linear programming）就是一类典型的约束优化，其解决的问题通常是在有限的成本约束下取得最大的收益。
    
    约束优化问题通常比无约束优化问题更加复杂，但通过拉格朗日乘子（Lagrange multiplier）的引入可以将含有 $n$ 个变量和 $k$ 个约束条件的问题转化为含有 $(n + K)$ 个变量的无约束优化问题。

- 梯度下降法（gradient descent）是求解无约束优化问题最常用的方法，梯度下降法就是沿着目标函数值下降最快的方向寻找最小值

    在数学上，梯度的方向是目标函数导数（derivative）的反方向。当函数的输入为向量时，目标函数的图象就变成了高维空间上的曲面，这时的梯度就是垂直于曲面等高线并指向高度增加方向的向量，也就携带了高维空间中关于方向的信息。而要让目标函数以最快的速度下降，就需要让自变量在负梯度的方向上移动。这个结论翻译成数学语言就是“多元函数沿其负梯度方向下降最快”，这也是梯度下降法的理论依据。
    
    在梯度下降算法中，另一个重要的影响因素是步长，也就是每次更新 $f(\mathbf{x})$ 时 $x$ 的变化值。较小的步长会导致收敛过程较慢，当 $f(\mathbf{x})$ 接近最小值点时，步长太大反而会导致一步迈过最小值点，正所谓“过犹不及”。因而在梯度下降法中，步长选择的整体规律是逐渐变小的。

- 批处理模式（batch processing）：即计算出在每个样本上目标函数的梯度，再将不同样本的梯度进行求和，求和的结果作为本次更新中目标函数的梯度。在批处理模式中，每次更新都要遍历训练集中所有的样本，因而运算量较大。

- 随机梯度下降法（stochastic gradient descent），它在每次更新中只使用一个样本，下一次更新再使用另外一个样本，在不断迭代的更新过程中实现对所有样本的遍历。事实表明当训练集的规模较大时，随机梯度下降法的性能更佳。

    梯度下降法只用到了目标函数的一阶导数（first-order derivative），并没有使用二阶导数（second-order derivative）。一阶导数描述的是目标函数如何随输入的变化而变化，二阶导数描述的则是一阶导数如何随输入的变化而变化，提供了关于目标函数曲率（curvature）的信息。曲率影响的是目标函数的下降速度。当曲率为正时，目标函数会比梯度下降法的预期下降得更慢；反之，当曲率为负时，目标函数则会比梯度下降法的预期下降得更快。

    梯度下降法不能利用二阶导数包含的曲率信息，只能利用目标函数的局部性质，因而难免盲目的搜索中。已知目标函数可能在多个方向上都具有增加的导数，意味着下降的梯度具有多种选择。但不同选择的效果显然有好有坏。
    
    梯度下降法无法获知关于导数的变化信息，也就不知道应该探索导数长期为负的方向。由于不具备观察目标函数的全局视角，在使用中梯度下降法就会走出一些弯路，导致收敛速度变慢。而二阶导数所包含的全局信息能够为梯度下降的方向提供指导，进而获得更优的收敛性。
    
    如果将二阶导数引入优化过程，得到的典型方法就是牛顿法（Newton's method）。在牛顿法中，目标函数首先被泰勒展开，写成二阶近似的形式（相比之下，梯度下降法只保留了目标函数的一阶近似）。此时再对二阶近似后的目标函数求导，并令其导数等于 0，得到的向量表示的就是下降最快的方向。相比于梯度下降法，牛顿法的收敛速度更快。
    
    不管是利用一阶导数的梯度下降法，还是利用二阶导数的牛顿法，其寻找最小值点的基本思想都是先确定方向，再确定步长，因而统称为“线性搜索方法”（line search）。
    
    还有一类算法，其寻找最小值点的基本思路是先确定步长，以步长为参数划定一个区域，再在这个区域内寻找最快下降的方向。这类算法被称为“置信域方法”（trust region）。
    
    具体来说，置信域算法的运行过程如下：设定一个置信域半径 $s$ ，并在以当前点为中心、以 $s$ 为半径的封闭球形区域作为置信域，在置信域内寻找目标函数的二次近似模型的最优点，最优点和当前点之间的距离就是计算出来的备选位移。
    
    在备选位移上，如果目标函数的二次近似产生了充分的下降，就将当前点移动到计算出的最优点，则继续按此规则迭代计算下去，并可以适当增加 $s$ ；如果目标函数的近似下降不够理想，则说明步子跨得太大，需要缩小 $s$ 并计算出新的备选位移，直到满足终止条件。


- 启发式算法（heuristics）的灵感来源于 20 世纪 50 年代诞生的仿生学，它将生物进化等自然现象的机理应用于现实世界复杂问题的优化之中，并取得了不俗的效果。

    启发式算法的实例包括模拟生物进化规律的遗传算法（geneticalgorithm）、模拟统计物理中固体结晶过程的模拟退火算法（simulated annealing）、模拟低等动物产生集群智能的蚁群算法（ant colony optimization）等等。

- 拉格朗日函数（Lagrange multiplier）：最简单的形式如下

    $$L(x, y, \lambda) = f(x, y) + \lambda \varphi(x, y)$$
    
    $f(x, y)$ 为目标函数， $\varphi(x, y)$ 则为等式约束条件， $\lambda$ 是拉格朗日乘数。
    
    从数学意义上讲，由原目标函数和约束条件共同构成的拉格朗日函数与原目标函数具有共同的最优点集和共同的最优目标函数值，从而保证了最优解的不变性。

- 蒙特卡罗算法（monte carlo method）：采样越多，越近似最优解

- 拉斯维加斯算法（las vegas algorithm）：采样越多，越有机会找到最优解

**Reference**


## 03 数学基础 | 窥一斑而知全豹：数理统计

- 数理统计（mathematical statistics）根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断

    在数理统计中，可用的资源是有限的数据集合，这个有限数据集被称为样本（sample）。相应地，观察对象所有的可能取值被称为总体（population）
    
    数理统计的任务就是根据样本推断总体的数字特征。样本通常由对总体进行多次独立的重复观测而得到，这保证了不同的样本值之间相互独立，并且都与总体具有相同的分布

- 概率论作用的前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断

- 统计量本身是一个随机变量，用来进行统计推断的工具。样本均值和样本方差是两个最重要的统计量：

    样本均值：${\bar X} = \dfrac{1}{n} \sum\limits_{i = 1}^{n} X_i$
    
    样本方差：$S ^ 2 = \dfrac{1}{n - 1} \sum\limits_{i = 1}^{n} (X_i - {\bar X}) ^ 2$

- 统计推断的基本问题可以分为两大类：参数估计（estimation theory）和假设检验（hypothesis test）

- 参数估计的对象是总体的某个参数，假设检验的对象则是关于总体的某个论断，即关于总体的假设。

- 参数估计是通过随机抽取的样本来估计总体分布的方法，又可以进一步划分为点估计（point estimation）和区间估计（interval estimation）。

    在已知总体分布函数形式，但未知其一个或者多个参数时，借助于总体的一个样本来估计未知参数的取值就是参数的点估计。点估计的核心在于构造合适的统计量 $\theta$ ，并用这个统计量的观察值作为未知参数 $\theta$ 的近似值。
    
    点估计的具体方法包括矩估计法（method of moments）和最大似然估计法（maximum likelihood estimation）。
    
    矩表示的是随机变量的分布特征， $k$ 阶矩的定义为随机变量的 $k$ 次方的均值，即 $E(X^k)$ 。矩估计法的思想在于用样本的 $k$ 阶矩估计总体的 $k$ 阶矩，其理论依据在于样本矩的函数几乎处处收敛于总体矩的相应函数，这意味着当样本的容量足够大时，几乎每次都可以根据样本参数得到相应总体参数的近似值。
    
    相对于基于大数定律的矩估计法，最大似然估计法源于频率学派看待概率的方式。对最大似然估计的直观理解是：既然抽样得到的是已有的样本值，就可以认为取到这一组样本值的概率较大，因而在估计参数 $\theta$ 的时候就需要让已有样本值出现的可能性最大。
    
    在最大似然估计中，似然函数被定义为样本观测值出现的概率，确定未知参数的准则是让似然函数的取值最大化，也就是微积分中求解函数最大值的问题。由于不同的样本值之间相互独立，因而似然函数可以写成若干概率质量函数 / 概率密度函数相乘的形式，并进一步转化为对数方程求解。
    
    矩估计法和最大似然估计法代表了两种推断总体参数的思路，但对于同一个参数，用不同的估计方法求出的估计量很可能存在差异，这就引出了如何对估计量进行评价的问题。在实际应用中，估计量的评价通常要考虑以下三个基本标准。
    
    **无偏性**：估计量的数学期望等于未知参数的真实值；
    
    **有效性**：无偏估计量的方差尽可能小；
    
    **一致性**：当样本容量趋近于无穷时，估计量依概率收敛于未知参数的真实值。
    
    以上三个要求构成了对点估计量的整体判定标准。无偏性意味着给定样本值时，根据估计量得到的估计值可能比真实值更大，也可能更小。但如果保持估计量的构造不变，而是进行多次重新抽样，每次都用新的样本计算估计值，那么这些估计值与未知参数真实值的偏差在平均意义上等于 0，这意味着不存在系统误差。
    
    虽然估计值与真实值之间的偏差不可避免，但个体意义上的偏差越小意味着估计的性能越精确，有效性度量的正是估计量和真实值之间的偏离程度。而偏离程度不仅仅取决于估计量的构造方式，还取决于样本容量的大小，一致性考虑的就是样本容量的影响。一致性表示的是随着样本容量的增大，估计量的值将稳定在未知参数的真实值上。不具备一致性的估计量永远无法将未知参数估计得足够精确，因而是不可取的。
    
    对估计量的判别标准涉及了估计误差的影响，这是和估计值同样重要的参量。在估计未知参数 $\theta$ 的过程中，除了求出估计量，还需要估计出一个区间，并且确定这个区间包含 $\theta$ 真实值的可信程度。在数理统计中，这个区间被称为置信区间（confidence interval），这种估计方式则被称为区间估计。
    
    置信区间可以用如下的方式直观解释：对总体反复抽样多次，每次得到容量相同的样本，则根据每一组样本值都可以确定出一个置信区间 $(\underline \theta ,\overline \theta)$ ，其上界和下界是样本的两个统计量，分别代表了置信上限和置信下限。
    
    每个置信区间都存在两种可能性：包含 $\theta$ 的真实值或不包含 $\theta$ 的真实值。如果对所有置信区间中包含 $\theta$ 真实值的比率进行统计，得到的比值就是置信水平。因此，区间估计相当于在点估计的基础上进一步提供了取值范围和误差界限，分别对应着置信区间和置信水平。

- 假设检验：假设包含原假设 $H_0$ 和备择假设 $H_1$ ；检验的过程就是根据样本在 $H_0$ 和 $H_1$ 之间选择一个接受的过程。
  
    假设错误形式分为两种：第 I 类错误对应假设 $H_0$ 为真但是被拒绝的情况，也就是“弃真”类型的错误；第 II 类错误对应假设 $H_0$ 不真但是被接受的情况，也就是“取伪”类型的错误。
    
    假设检验的思维方式建立在全称命题只能被证伪不能被证实的基础上。要证明原假设 $H_0$ 为真，更容易的方法是证明备择假设 $H_1$ 为假，因为只要能够举出一个反例就够了。但在假设检验中，反例并非绝对意义上对假设的违背，而是以小概率事件的形式出现。
    
    在数理统计中，发生概率小于 1% 的事件被称作小概率事件，在单次实验中被认为是不可能发生的。如果在一次观测得到的样本中出现了小概率事件，那么就有理由认为这不是真正意义上的小概率事件，原始的假设也就此被推翻。如果是备择假设被推翻，就意味着接受原假设；反之，如果是原假设被推翻，则意味着拒绝原假设。
    
    从数理统计的角度看，监督学习算法的任务就是在假设空间中搜索能够针对特定问题做出良好预测的假设。学习器通过对测试数据集的学习得到具有普适性的模型，这个模型适用于不属于测试集的新样本的能力被称为泛化能力。显然，泛化能力越强，学习器就越好。
    
    假设检验的作用就在于根据学习器在测试集上的性能推断其泛化能力的强弱，并确定所得结论的精确程度，可以进一步推广为比较不同学习器的性能。由于度量学习器性能的常用指标是错误率，假设检验中的假设就是对学习器的泛化错误率的推断，推断的依据就是在测试数据集上的测试错误率。

- 泛化误差的构成可以分为三部分：偏差（bias）、方差（variance）和噪声（noise）。
  
    偏差表示算法预测值和真实结果之间的偏离程度，刻画的是模型的欠拟合特性；
    
    方差表示数据的扰动对预测性能的影响，刻画的是模型的过拟合特性；
    
    噪声表示在当前学习任务上能够达到的最小泛化误差，刻画的是任务本身的难度。
    
    对任何实际的模型来说，偏差和方差都难以实现同时优化，反映出欠拟合与过拟合之间难以调和的矛盾。

**总结**

- 数理统计的任务是根据可观察的样本反过来推断总体的性质；
- 推断的工具是统计量，统计量是样本的函数，是个随机变量；
- 参数估计通过随机抽取的样本来估计总体分布的未知参数，包括点估计和区间估计；
- 假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用于估计机器学习模型的泛化错误率。

**术语**
    
- 大数定律：大量随机变量的均值收敛于其数学期望，描述了大量随机现象平均结果的稳定性

- 中心极限定理：n 个相互独立同分布的随机变量之和的分布近似于正态分布

**Reference**


## 02 数学基础 | 月有阴晴圆缺，此事古难全：概率论

- 概率论（probability theory）关注的焦点是无处不在的可能性。对随机事件发生的可能性进行规范的数学描述就是概率论的公理化过程。概率的公理化结构体现出的是对概率本质的一种认识
- 频率学派认为假设是客观存在且不会改变的，即存在固定的先验分布，只是作为观察者的我们无从知晓。在计算具体事件的概率时，要先确定概率分布的类型和参数，以此为基础进行概率推演
- 贝叶斯学派则认为固定的先验分布是不存在的，参数本身也是随机数。换言之，假设本身取决于观察结果，是不确定并且可以修正的。数据的作用就是对假设做出不断的修正，使观察者对概率的主观认识更加接近客观实际
- 概率的估计有两种方法：最大似然估计法（maximum likelihood estimation）和最大后验概率法（maximum a posteriori estimation），两者分别体现出频率学派和贝叶斯学派对概率的理解方式

    最大似然估计法的思想是使训练数据出现的概率最大化，依此确定概率分布中的未知参数，估计出的概率分布也就最符合训练数据的分布
    最大后验概率法的思想则是根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最可能的未知参数取值作为估计值
    在估计参数时，最大似然估计法只需要使用训练数据，最大后验概率法除了数据外还需要额外的信息，就是贝叶斯公式中的先验概率

**总结**

- 概率论关注的是生活中的不确定性或可能性；
- 频率学派认为先验分布是固定的，模型参数要靠最大似然估计计算；
- 贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算；
- 正态分布是最重要的一种随机变量的分布。

**术语**

- 联合概率（joint probability）：假定有两个随机事件 A 和 B，联合概率表示 A 和 B 两个事件共同发生的概率

    $$P(AB)$$

- 条件概率（conditional probability）：假定有两个随机事件 A 和 B，条件概率表示事件 A 在事件 B 已经发生的条件下发生的概率

    $$P(A|B) = \dfrac{P(AB)}{P(B)}$$
    
    条件概率是根据已有信息对样本空间进行调整后得到的新的概率分布
    
    如果两个事件的发生互不影响，即两者相互独立，那么联合概率等于两个事件各自概率的乘积：
    
    $$P(AB) = P(A) \cdot P(B)$$
    
    对于相互独立的事件，条件概率就是自身的概率：
    
    $$P(A|B) = P(A)$$

- 全概率公式（law of total probability）：根据条件概率，全概率公式作用在于将复杂事件的概率求解转化为在不同情况下发生的简单事件的概率求和

    $$P(A) = \sum_{i = 1}^{N}P(A|B_i)\cdot P(B_i)$$

- 贝叶斯定理（Bayes' theorem）：对全概率公式稍作整理，就演化出了求解“逆概率”这一重要问题。所谓“逆概率”解决的是在事件结果已经确定的条件下 $P(A)$，推断各种假设发生的可能性 $P(B_i|A)$

    $$P(B_i|A)=\frac{P(A|B_i)\cdot P(B_i)}{\sum _{j=1}^{N} P(A|B_j)\cdot P(B_j)}$$

    $$P(H|D) = \dfrac{P(D|H) \cdot P(H)}{P(D)}$$
    
    $P(H)$ 被称为先验概率（prior probability），即预先设定的假设成立的概率
    
    $P(D|H)$ 称为似然概率（likelihood function），是在假设成立的前提下观测到结果的概率
    
    $P(H|D)$ 称为后验概率（posterior probability），即在观测到结果的前提下假设成立的概率

- 随机变量（random variable）：根据取值空间的不同，随机变量可以分成两类：离散型随机变量（discrete random variable）和连续型随机变量（continuous random variable）

    概率质量函数（probability mass function）离散变量的每个可能的取值都具有大于 0 的概率，取值和概率之间一一对应的关系就是离散型随机变量的分布律
    
    概率密度函数（probability density function）在连续型随机变量上的概率质量函数，概率密度函数体现的并非连续型随机变量的真实概率，而是不同取值可能性之间的相对关系

- 分布（distribution）：

    两点分布（Bernoulli distribution）：适用于随机试验的结果是二进制的情形，事件发生 / 不发生的概率分别为 $p/(1 - p)$。任何只有两个结果的随机试验都可以用两点分布描述，抛掷一次硬币的结果就可以视为等概率的两点分布。

    二项分布（Binomial distribution）：将满足参数为 p 的两点分布的随机试验独立重复 n 次，事件发生的次数即满足参数为 (n,p) 的二项分布。二项分布的表达式可以写成 $P(X = k) = C^n_k \cdot p ^ k \cdot (1 - p) ^ {(n - k)}, 0 \le k \le n$

    泊松分布（Poisson distribution）：放射性物质在规定时间内释放出的粒子数所满足的分布，参数为 $\lambda$ 的泊松分布表达式为 $P(X = k) = \lambda ^ k \cdot e ^ {-\lambda} / (k!)$。当二项分布中的 n 很大且 p 很小时，其概率值可以由参数为 $\lambda = np$ 的泊松分布的概率值近似。

    均匀分布（uniform distribution）：在区间 (a, b) 上满足均匀分布的连续型随机变量，其概率密度函数为 1 / (b - a)，这个变量落在区间 (a, b) 内任意等长度的子区间内的可能性是相同的。
    
    指数分布（exponential distribution）：满足参数为 $\theta$ 指数分布的随机变量只能取正值，其概率密度函数为 $e ^ {-x / \theta} / \theta, x > 0$ 。指数分布的一个重要特征是无记忆性：即 $P(X > s + t | X > s) = P(X > t)$ 
    
    正态分布（normal distribution）：参数为正态分布的概率密度函数为 $f(x) = \dfrac{1}{\sqrt{2\pi}\sigma} \cdot e ^ {-\frac{(x - \mu) ^ 2}{2\sigma ^ 2}}$ 。当 $\mu = 0, \sigma = 1$ 时，上式称为标准正态分布。正态分布是最常见最重要的一种分布，自然界中的很多现象都近似地服从正态分布。

- 期望（expected value）：期望即均值，体现的是随机变量可能取值的加权平均，即根据每个取值出现的概率描述作为一个整体的随机变量的规律，描述的是集中程度（center）
    
- 方差（variance）：方差表示的则是随机变量的取值与其数学期望的偏离程度。方差较小意味着随机变量的取值集中在数学期望附近，方差较大则意味着随机变量的取值比较分散，描述的是离散程度（dispersion）
    
- 协方差（covariance）：协方差度量了两个随机变量之间的线性相关性，即变量 Y 能否表示成以另一个变量 X 为自变量的 $aX+b$ 的形式

    数学期望和方差描述的是单个随机变量的数字特征，协方差和相关系数描述得失两个随机变量之间的相互关系
    
    相关系数（correlation coefficient）是一个绝对值不大于 1 的常数，它等于 1 意味着两个随机变量满足完全正相关，等于 -1 意味着两者满足完全负相关，等于 0 则意味着两者不相关
    
    无论是协方差还是相关系数，刻画的都是线性相关的关系。如果随机变量之间的关系满足 $Y = X ^ 2$ ，这样的非线性相关性就超出了协方差的表达能力

**Reference**

- [人工智能各学派的认知观](https://books.google.com.hk/books?id=BzZfhKffXBMC&pg=PA6)
- [概率论与信息论](https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/)

## 01 数学基础 | 九层之台，起于累土：线性代数

- 万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察
- 线性代数是用虚拟数字世界表示真实物理世界的工具
- 在线性空间中，任意一个向量代表的都是 n 维空间中的一个点；反过来，空间中的任意点也都可以唯一地用一个向量表示
- 点的变化对应着向量的线性变换（linear transformation），而描述对象变化抑或向量变换的数学语言，正是矩阵

    线性空间中，变化的实现有两种方式：一是点本身的变化，二是参考系的变化；对参考系施加变换的方法，就是让表示原始参考系的矩阵与表示变换的矩阵相乘

**总结**

- 线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态的特性；
- 向量的实质是 n 维线性空间中的静止点；
- 线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示；
- 矩阵的特征值和特征向量描述了变化的速度与方向。

**术语**

- **集合（set）**：具有某种特性的事物的整体
- **标量（scalar）**、**向量（vector）**、**矩阵（matrix）**、**张量（tensor）**
- **范数（norm）**：对向量大小的度量，其作用是将向量映射为一个非负的数值

    $L ^ p$ 范数： ${\left| {\bf{x}} \right|_p} = {\left( {\sum\limits_i {{{\left| {{x_i}} \right|}^p}} } \right)^{\frac{1}{p}}}$

    $L ^ 1$ 范数计算的是向量所有元素绝对值的和， $L ^ 2$ 范数计算的是通常意义上的向量长度， $L ^ {\infty}$ 范数计算的则是向量中最大元素的取值

- **内积（inner product）**：对应元素乘积的求和。内积能够表示两个向量之间的相对位置，即向量之间的夹角，计算的是两个向量之间的关系

    $$\left\langle {{\bf{x,y}}} \right\rangle  = \sum\limits_i {{x_i} \cdot {y_i}}$$

    一种特殊的情况是内积为 0，即 $\left\langle \bf{x,y} \right\rangle = 0$ 。在二维空间上，这意味着两个向量的夹角为 90 度，即相互垂直。而在高维空间上，这种关系被称为正交（orthogonality）。如果两个向量正交，说明他们线性无关，相互独立，互不影响

- **空间（space）**：具有一些附加结构的集合（a set with some added structure）；结构（structure）是数学对象（mathematical object）以某种方式附加（或关联）附加到该集合，它赋予了一些额外的含义或意义（特征或特性）

- **正交基（orthogonal basis）**：内积空间中，元素两两正交的基，称基中的元素为基向量；假若正交基中基向量的 $L ^ 2$ 范数（模）都是单位长度 1，这组正交基就是 **标准正交基（orthonormal basis）**

    正交基的作用就是给内积空间定义出经纬度，⼀旦描述内积空间的正交基确定了，向量和点之间的对应关系也就随之确定。描述内积空间的正交基并不唯一，如二维空间中的平面直角坐标系和极坐标系就对应了两组不同的正交基，也代表了两种实用的描述方式

- **特征值（eigenvalue）**、**特征向量（eigenvector）**：描述矩阵的⼀对重要参数。其中 $A$ 是 $n\times n$ 矩阵， $x$ 是 $n$ 维向量，则 $\lambda$ 是矩阵 $A$ 的一个特征值，而 $x$ 是矩阵 $A$ 的特征值 $\lambda$ 所对应的特征向量，其关系如下：

    $${\bf{Ax}} = \lambda {\bf{x}}$$

    矩阵代表了向量的变换，其效果通常是对原始向量同时施加方向变化和尺度变化

    可对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，也就是只有伸缩的效果而没有旋转的效果，对于给定的矩阵来说，这类特殊的向量就是矩阵的特征向量，特征向量的尺度变化系数就是特征值

    矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向

- **特征值分解（eigendecomposition）**：求解给定矩阵的特征值和特征向量的过程

    能够进行特征值分解的矩阵必须是 n 维矩阵

- **奇异值分解（singular-value decomposition）**：$M$ 是 $m\times n$ 矩阵，$U$ 是 $MM^T$ 特征向量张成一个 $m\times m$ 的矩阵，$V$ 是 $M^TM$ 特征向量张成一个 $n\times n$ 的矩阵，$\Sigma$ 是特征值开平方张成一个 $m\times n$ 矩阵

    奇异值分解是特征值分解在任意矩阵上的推广

    $$A = W\Sigma W^{-1} = W\Sigma W^T \Rightarrow M = U\Sigma V^T$$
    
    $$M=U\Sigma V^T \Rightarrow MV=U\Sigma V^TV \Rightarrow MV=U\Sigma \Rightarrow  Mv_i = \sigma_i u_i  \Rightarrow  \sigma_i =  Mv_i / u_i$$

**Reference**

- [线性代数](https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/)
- [数学中关于集合、空间、数域概念的关系？](https://www.zhihu.com/question/41193747/answer/94981893)
- [奇异值的物理意义是什么？](https://www.zhihu.com/question/22237507)
- [奇异值分解(SVD)原理与在降维中的应用](http://www.cnblogs.com/pinard/p/6251584.html)

## Reference

- [人工智能基础课 - 通俗易懂的人工智能入门课](https://time.geekbang.org/column/intro/62)
